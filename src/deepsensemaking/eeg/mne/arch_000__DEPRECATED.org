

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes


        def concatenate_data_by_subject(self,signals0="signals0",reload=True,cleanup=True):
            """
            Merge datasets
            This is a quick-fix (quick-hack) implemented here
            because in the initial pipeline design each run was
            preprocessed/analysed separately, however

            TODO FIXME add information about RUN in the EVENTS description.
            TODO FIXME CONSIDER better use such merging at the stage of getting paths

            Example use:
              data = DS0.dataBase
              data.concatenate_data_by_subject(signals0="raw0",)
              data.concatenate_data_by_subject(signals0="epochs0",)


            """
            ## Produce data dictionary
            ## - keys are based on subject codes
            ## - values contain lists that in turn contain
            ##   indices of datasets for a given subject
            self.group_by_subject = OrderedDict()
            for idx0,item0 in enumerate(self.data):
                self.BATCH.logger.info (space0[1]+"looking at dataset {:03d}: {}".format(idx0,item0.locs.of_stem))
                self.BATCH.logger.debug(space0[2]+"sub        : {}".format(item0.sub))
                self.BATCH.logger.debug(space0[2]+"run        : {}".format(item0.run))
                self.BATCH.logger.debug(space0[2]+"concatBool : {}".format(item0.concatBool))
                ## check if item is an original recording
                ## not a concatenation
                if not item0.concatBool:
                    ## check if subject for this item is already used as a key
                    if not item0.sub in list(self.group_by_subject.keys()):
                        self.group_by_subject[item0.sub] = list()

                    ## append index for this item to list sored in dict value
                    self.group_by_subject[item0.sub].append(idx0)

            ## Construct new datasets
            ## - iterating over subjects
            for sub0,list0 in self.group_by_subject.items():
                ## - get the first item
                ## - use it as a templete for the contcatenated dataset
                item0 = self.data[list0[0]]
                ## - make run-000
                ##   - TODO FIXME add regex here to facilitate
                if_path = pathlib.Path(str(item0.locs.if_path).replace("_run-001","_run-000",))
                self.data.append(
                    self.DataSet(
                        BATCH      = self.BATCH,
                        INSP       = self.INSP ,
                        item       = if_path,
                        idx        = len(self.data),
                        concatBool = True,
                        concatDict = OrderedDict(),
                    )
                )
                self.BATCH.logger.info (space0[1]+"ADDED DataSet: {}".format(repr(str(self.data[-1]))))
                self.BATCH.logger.info (space0[2]+"BASED on: {}"     .format(repr(str(item0))))
                ## - iterating over signals for current subject
                for idx1 in list0[:]:
                    item1 = self.data[idx1]
                    if signals0 in item1.data.keys():
                        self.BATCH.logger.info (space0[3]+"INCLUDING {} from DataSet: {}".format(repr(str(signals0)),item1))
                        self.data[-1].concatDict[str(item1.locs.of_stem)] = dc(item1.data[signals0])

                concatList = list(self.data[-1].concatDict.values())
                if concatList:
                    if isinstance(concatList[0], mne.epochs.BaseEpochs):
                        self.data[-1].data[signals0] = mne.concatenate_epochs(dc(concatList))

                    elif isinstance(concatList[0], mne.io.brainvision.brainvision.RawBrainVision):
                        self.data[-1].data[signals0] = mne.concatenate_raws(dc(concatList))

                    else:
                        raise NotImplementedError




#+END_SRC


#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes
"""
If present BAD channels SHOULD be loaded during the SECOND RUN
"""
self.check_BAD_chans_file(
    raw0 = "raw0",
)
self.add_actual_reference(
    raw0          = "raw0",
    ref_chans_OLD = self.BATCH.dataBase.setup["chans"]["refs"],
)

                self.read_raw_data(
                    raw0    = "raw0",
                    preload = True,
                    verbose = None,
                )
                self.check_chans_number(
                    raw0 = "raw0",
                )



                ## currently BAD spans checkup is deprecated,
                ## instead in the following steps epoched data
                ## will be cleaned using ICA and amplitude thresholds...

                self.BATCH.logger.info("checking for bad spans")
                self.check_for_BAD_spans_file()



                ## currently manual inspection of RAW (continous)
                ## data is deprecated
                ## instead, epoched data will be inspected
                ## in following steps and
                ## it will be cleaned using ICA and amplitude thresholds...
                ## ALSO: BAD spans data should not be loaded OR written

                if sys.stdout.isatty(): plt.close("all")
                self.BATCH.logger.info("plot RAW data timeseries for MANUAL INSPECTION")
                self.plot_raw_data_timeseries(total=False,exclude=False,)
                if ASK & sys.stdout.isatty(): input(temp_continue)
                if sys.stdout.isatty(): plt.close("all")
                self.BATCH.logger.info("save BAD spans after MANUAL INSPECTION ")
                self.export_BAD_spans_info()


                self.check_BAD_epochs_file(
                    epochs0 = "epochs0",
                )


                self.drop_BAD_epochs()




#+END_SRC




#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes


            def check_BAD_chans_file(
                    self,
                    raw0,
            ):
                self.BATCH.logger.info(
                    space0[0]+"RUNNING: {}.{}".format(
                        ".".join(self.INSP),
                        str(whoami()),
                ))
                self.BATCH.logger.info (space0[1]+"checking BAD channels information file...")
                self.BATCH.logger.info (space0[1]+"processing: {}".format(repr(str( self ))))
                self.BATCH.logger.info (space0[1]+"raw0: {}"      .format(repr(str( raw0 ))))
                bad_names    = list()
                of_BAD_chans = self.locs.of_BAD_chans
                self.BATCH.logger.debug(space0[1]+"looking for of_BAD_chans: {}".format(repr(str(of_BAD_chans))))
                if os.path.exists(of_BAD_chans):
                    self.BATCH.logger.info(space0[1]+"found BAD channels file...")
                    with open(of_BAD_chans) as fh:
                        for line in fh:
                            line = line.split('#',1,)[0].strip()
                            if line:
                                bad_names.append(line)

                else:
                    self.BATCH.logger.info(space0[1]+"BAD channels file NOT found...")

                if bad_names:
                    self.BATCH.logger.info (space0[1]+"bad_names: {}".format(str(bad_names)))
                    self.BATCH.logger.info(space0[1]+"adding BAD channels informtion to raw data")
                    self.data[raw0].info['bads'] += bad_names

                    self.BATCH.logger.info(space0[1]+"uniquifying bad channels info")
                    self.data[raw0].info['bads'] = list(set(self.data[raw0].info['bads']))

                else:
                    self.BATCH.logger.warning(space0[1]+"bad_names: {}".format(str(bad_names)))








            def export_BAD_spans_info(self,raw0="raw0"):
                self.BATCH.logger.info(
                    space0[0]+"RUNNING: {}.{}".format(
                        ".".join(self.INSP),
                        str(whoami()),
                ))
                self.BATCH.logger.info (space0[1]+"exporting BAD spans annotation data to a CSV file...")
                self.BATCH.logger.info (space0[1]+"processing: {}".format(repr(str( self    ))))
                self.BATCH.logger.info (space0[1]+"raw0: {}"      .format(repr(str( raw0    ))))

                self.BATCH.logger.info (space0[1]+"extracting BAD spans from RAW data annotations...")
                self.BATCH.logger.info (space0[1]+"for synchronization putpose 0th annot is also included...")
                bads_annot1 = [0]+[ii for ii,an in enumerate(self.data[raw0].annotations) if an['description'].lower().startswith("bad")]

                of_suff0 = ""
                of_suff0 = ".".join([of_suff0,str(whoami()),raw0])
                of_suff1 = ".".join([of_suff0,"annots1","BAD_spans","csv"])
                of_suff2 = ".".join([of_suff0,"annots2","NEW_check","csv"])
                of_name0 = str(self.locs.of_BAD_spans)
                of_name1 = str(self.locs.of_base.with_suffix(of_suff1))
                of_name2 = str(self.locs.of_base.with_suffix(of_suff2))

                self.BATCH.logger.info (space0[1]+"of_name0: {}".format(repr(str(of_name0))))
                self.BATCH.logger.info (space0[1]+"of_name1: {}".format(repr(str(of_name1))))
                self.BATCH.logger.info (space0[1]+"of_name2: {}".format(repr(str(of_name2))))

                self.data[raw0].annotations[bads_annot1].save(of_name0)
                self.data[raw0].annotations[bads_annot1].save(of_name1)
                self.data[raw0].annotations[          :].save(of_name2)




            def check_for_BAD_spans_file(
                    self,
                    raw0    = "raw0",
                    annots1 = "annots1",
            ):
                self.BATCH.logger.info(
                    space0[0]+"RUNNING: {}.{}".format(
                        ".".join(self.INSP),
                        str(whoami()),
                ))
                self.BATCH.logger.info (space0[1]+"checking for BAD spans...")
                self.BATCH.logger.info (space0[1]+"processing: {}".format(repr(str( self    ))))
                self.BATCH.logger.info (space0[1]+"raw0: {}"      .format(repr(str( raw0    ))))
                self.BATCH.logger.info (space0[1]+"annots1: {}"   .format(repr(str( annots1 ))))

                of_BAD_spans = self.locs.of_BAD_spans
                self.BATCH.logger.debug(space0[1]+"looking for of_BAD_spans: {}".format(repr(str(of_BAD_spans))))

                # of_annot1 = str(self.locs.of_base.with_suffix(".raw0.annots1.bad_spans.csv"))
                # self.BATCH.logger.info (space0[1]+"looking for of_annot1: "    + str(of_annot1))

                if os.path.exists(of_BAD_spans):
                    self.BATCH.logger.info (space0[2]+"found BAD span annottions file!")
                    self.BATCH.logger.info (space0[2]+"getting BAD span annots data")
                    self.BATCH.logger.info (space0[2]+"EXEC: {}" .format("mne.read_annotations()"))
                    self.data[annots1]              = OrderedDict()
                    self.data[annots1]["bad_spans"] = mne.read_annotations(
                        of_BAD_spans,
                    )
                    self.BATCH.logger.info (space0[2]+"adding BAD span annots to {} data".format(repr(str(raw0))))
                    self.BATCH.logger.info (space0[2]+"EXEC: {}[{}].{}".format(".".join(self.INSP),repr(raw0),"set_annotations(OLD+NEW)"))
                    self.data[raw0].set_annotations(
                        self.data[raw0].annotations + self.data[annots1]["bad_spans"],
                    )
                    self.BATCH.logger.info (space0[1]+"ALL GOOD...")
                else:
                    self.BATCH.logger.info (space0[1]+"file not found")
                    self.BATCH.logger.info (space0[1]+"annots1 were NOT updated")
                    self.BATCH.logger.info (space0[1]+"it is OK during the first run")






            def check_BAD_epochs_file(
                    self,
                    epochs0,
            ):
                self.BATCH.logger.info(
                    space0[0]+"RUNNING: {}.{}".format(
                        ".".join(self.INSP),
                        str(whoami()),
                ))
                self.BATCH.logger.info (space0[1]+"checking BAD epochs information file...")
                self.BATCH.logger.info (space0[1]+"processing: {}".format(repr(str( self    ))))
                self.BATCH.logger.info (space0[1]+"epochs0: {}"   .format(repr(str( epochs0 ))))

                bad_epochs = list()
                of_BAD_epochs   = self.locs.of_BAD_epochs
                self.BATCH.logger.debug(space0[2]+"looking for of_BAD_epochs: {}".format(repr(str(of_BAD_epochs))))
                if os.path.exists(of_BAD_epochs):
                    self.BATCH.logger.info(space0[2]+"found bad epochs file...")
                    with open(of_BAD_epochs) as fh:
                        for line in fh:
                            line = line.split('#',1,)[0].strip()
                            if line:
                                bad_epochs.append(int(line))

                bad_epochs = list(set(bad_epochs))
                self.BATCH.logger.info (space0[2]+"bad_epochs: {}".format(repr(str(bad_epochs))))
                if bad_epochs:
                    self.BATCH.logger.info(space0[1]+"adding BAD epochs informtion to data")
                    self.data[epochs0].drop(bad_epochs)
                    ## TODO FIXME check if this truely operates inplace (in-place)



            def drop_BAD_epochs(
                    self,
                    epochs0 = "epochs0",
                    reject  = "setup",
                    flat    = "setup",

            ):
                self.BATCH.logger.info(
                    space0[0]+"RUNNING: {}.{}".format(
                        ".".join(self.INSP),
                        str(whoami()),
                ))
                self.BATCH.logger.info (space0[1]+"dropping epochs marked as BAD")
                self.BATCH.logger.info (space0[1]+"processing: {}".format(repr(str( self    ))))
                self.BATCH.logger.info (space0[1]+"epochs0: {}"   .format(repr(str( epochs0 ))))

                if reject == "setup":
                    reject = self.BATCH.dataBase.setup["params"]["reject"]

                if flat == "setup":
                    flat = self.BATCH.dataBase.setup["params"]["flat"]

                self.BATCH.logger.info (space0[1]+"reject: {}".format(repr(str( reject ))))
                self.BATCH.logger.info (space0[1]+"flat: {}"  .format(repr(str( flat   ))))

                ARGS = dict(
                    reject   = reject,
                    flat     = flat,
                )
                for line in str_dict(ARGS,space0[1]+"ARGS",max_level=0,max_len=42,tight=True,).split("\n"): self.BATCH.logger.info(line)
                self.data[epochs0].drop_bad(
                      **ARGS,
                )
                self.BATCH.logger.info (space0[1]+"DONE with epochs dropping...")
                ## TODO FIXME check if this truely operates inplace (in-place)










#+END_SRC





#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes


                # self.read_raw_fif(
                #     raw0    = "raw0",
                #     preload = True,
                #     verbose = None,
                # )


                """

                event_id = self.BATCH.dataBase.setup["events"]["dictX"]
                event_di = {val:key for key,val in event_id.items() }

                events0,events_dict0 = mne.events_from_annotations(
                    self.data["raw0"],
                    event_id = self.BATCH.dataBase.setup["events"]["dictX"]
                  )

                annot_from_events = mne.annotations_from_events(
                    events=events0,
                    event_desc=event_di,
                    sfreq=raw.info['sfreq'],
                    orig_time=raw.info['meas_date'],
                )
                raw.set_annotations(annot_from_events)


                # ########################
                # TODO FIXME USE COPY() TO TEST !!!
                # ########################




                # https://mne.tools/dev/auto_tutorials/intro/plot_20_events_from_raw.html
                for self.data["raw0"].annotations

                        of_suff = ".".join([of_suff,str(whoami()),key0])
                        of_suff = "-".join([of_suff,"epo.fif"])
                        of_name = self.locs.of_base.with_suffix(of_suff)
                        val0.save(of_name,overwrite=overwrite)

                """



#+END_SRC




#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

            def extract_PEAKS_from_evoked_to_dataframe(
                    self,
                    df0_peaks0,     # df0_peaks0 = "df0_peaks0",
                    evoked0,        # evoked0    = "evoked2",
                    chans0,         # chans0     = self.BATCH.dataBase.setup["chans"]["bund1"]["B1"],
                    bunds0,         # bunds0     = self.BATCH.dataBase.setup["chans"]["bund0"].items(),
                    timespans0,     # timespans0 = list(self.BATCH.dataBase.setup["time"]["spans0"].values()),
                    crude = False, # crude     = False,
            ):
            """

            CAUTION: this errors because of a bug in MNE that checks
            for positive/negative values in data but np.argmin/np.argmax
            is applied to signal between tmin and tmax


            """
                self.BATCH.logger.info(
                    space0[0]+"RUNNING: {}.{}".format(
                        ".".join(self.INSP),
                        str(whoami()),
                ))
                self.BATCH.logger.info (space0[1]+"extracting peaks from evoked for some data channels accross conditions")
                self.BATCH.logger.info (space0[1]+"processing: {}".format(repr(str( self        ))))
                self.BATCH.logger.info (space0[1]+"df0_peaks0: {}".format(repr(str( df0_peaks0      ))))
                self.BATCH.logger.info (space0[1]+"evoked0: {}"   .format(repr(str( evoked0     ))))
                self.BATCH.logger.info (space0[1]+"chans0: {}"    .format(repr(str( chans0      ))))
                self.BATCH.logger.info (space0[1]+"bunds0: {}"    .format(repr(str( bunds0      ))))
                self.BATCH.logger.info (space0[1]+"timespans0: {}".format(repr(str( timespans0  ))))
                self.data[df0_peaks0] = None
                df0 = pd.DataFrame(
                    [],
                    columns=["evoked0","quest0","cond0","chan0","tmin0","tmax0","mode0","chanX","latX","valX"],
                )
                for quest0 in self.data[evoked0].keys():
                    self.BATCH.logger.info (space0[1]+"PROC quest0: {}".format(repr(str( quest0 ))))
                    for idx0,(cond0,val0) in enumerate(self.data[evoked0][quest0].items()):
                        self.BATCH.logger.info (space0[2]+"cond0: {}".format(repr(str( cond0 ))))
                        for chan0 in chans0:
                            self.BATCH.logger.debug(space0[2]+"chan0: {}".format(repr(str( chan0 ))))
                            for timespan0 in timespans0:
                                tmin0 = timespan0[0]
                                tmax0 = timespan0[1]
                                for mode0 in ["pos","neg"]:
                                    smin0,smax0 = val0.time_as_index([tmin0,tmax0])
                                    chan_data = val0.copy().pick(chan0).data[0,smin0:smax0]
                                    if mode0 == "neg": chan_data = -chan_data
                                    chan_peak,_ = find_peaks(
                                        x            = chan_data,
                                        height       = None,
                                        threshold    = None,
                                        distance     = 4e4,
                                        prominence   = None,
                                        width        = None,
                                        wlen         = None,
                                        rel_height   = 0.5,
                                        plateau_size = None,
                                    )
                                    chanX = chan0
                                    latX  = val0.times[chan_peak+smin0]
                                    valX  = chan_data[chan_peak]
                                    latX  = latX[0] if latX.size > 0 else np.nan
                                    valX  = valX[0] if valX.size > 0 else np.nan
                                    if mode0 == "neg": valX = -valX
                                    if crude:
                                        try:
                                            chanX,latX,valX = val0.copy().pick(chan0).get_peak(
                                                ch_type          = "eeg",
                                                tmin             = tmin0,
                                                tmax             = tmax0,
                                                mode             = mode0,
                                                return_amplitude = True,
                                            )
                                        except ValueError:
                                            self.BATCH.logger.warning(space0[1]+"*** WARNING *** handling ValueError for {}".format(repr(str( chan0 ))))
                                            chanX,latX,valX = chan0,np.nan,np.nan

                                    df1 = pd.DataFrame(
                                        [[evoked0,quest0,cond0,chan0,tmin0,tmax0,mode0,chanX,latX,valX]],
                                        columns=["evoked0","quest0","cond0","chan0","tmin0","tmax0","mode0","chanX","latX","valX"],
                                    )
                                    df1["valX"] = df1["valX"]*1e6
                                    # df1["STEM"] = str(self.locs.of_stem)
                                    df1["SUB"]  = self.sub
                                    df1["SES"]  = self.ses
                                    df1["TASK"] = self.task
                                    df1["RUN"]  = self.run
                                    df0 = df0.append(df1)


                        inst0     = dc(val0)
                        bunds0idx = OrderedDict()
                        for key2,val2 in bunds0:
                            self.BATCH.logger.info (space0[2]+"bunds0: {}".format(repr(str( bunds0 ))))
                            self.BATCH.logger.info (space0[2]+"bunds0 key2: {}".format(repr(str( key2 ))))
                            self.BATCH.logger.info (space0[2]+"bunds0 val2: {}".format(repr(str( val2 ))))
                            bunds0idx[key2] = mne.pick_channels(inst0.info['ch_names'], val2 )

                        ## TODO FIXME CONSIDER avoiding code repetition and use combine_channels for every single channel
                        inst1 = mne.channels.combine_channels(
                            inst   = inst0,
                            groups = bunds0idx,
                            method = "mean",
                        )
                        for chan0 in inst1.info["ch_names"]:
                            self.BATCH.logger.debug(space0[2]+"chan0: {}".format(repr(str( chan0 ))))
                            for timespan0 in timespans0:
                                tmin0 = timespan0[0]
                                tmax0 = timespan0[1]
                                for mode0 in ["pos","neg"]:
                                    smin0,smax0 = inst1.time_as_index([tmin0,tmax0])
                                    chan_data = inst1.copy().pick(chan0).data[0,smin0:smax0]
                                    if mode0 == "neg": chan_data = -chan_data
                                    chan_peak,_ = find_peaks(
                                        x            = chan_data,
                                        height       = None,
                                        threshold    = None,
                                        distance     = 4e4,
                                        prominence   = None,
                                        width        = None,
                                        wlen         = None,
                                        rel_height   = 0.5,
                                        plateau_size = None,
                                    )
                                    chanX = chan0
                                    latX  = inst1.times[chan_peak+smin0]
                                    valX  = chan_data[chan_peak]
                                    latX  = latX[0] if latX.size > 0 else np.nan
                                    valX  = valX[0] if valX.size > 0 else np.nan
                                    if mode0 == "neg": valX = -valX
                                    if crude:
                                        try:
                                            chanX,latX,valX = inst1.copy().pick(chan0).get_peak(
                                                ch_type          = "eeg",
                                                tmin             = tmin0,
                                                tmax             = tmax0,
                                                mode             = mode0,
                                                return_amplitude = True,
                                            )
                                        except ValueError:
                                            self.BATCH.logger.warning(space0[1]+"*** WARNING *** handling ValueError for {}".format(repr(str( chan0 ))))
                                            chanX,latX,valX = chan0,np.nan,np.nan

                                    df1 = pd.DataFrame(
                                        [[evoked0,quest0,cond0,chan0,tmin0,tmax0,mode0,chanX,latX,valX]],
                                        columns=["evoked0","quest0","cond0","chan0","tmin0","tmax0","mode0","chanX","latX","valX"],
                                    )
                                    df1["valX"] = df1["valX"]*1e6
                                    # df1["STEM"] = str(self.locs.of_stem)
                                    df1["SUB"]  = self.sub
                                    df1["SES"]  = self.ses
                                    df1["TASK"] = self.task
                                    df1["RUN"]  = self.run
                                    df0 = df0.append(df1)

                di0 = self.BATCH.dataBase.setup["chans"]["bund0"]
                di1 = OrderedDict()
                for key0,val0 in di0.items():
                    for item in val0:
                        di1[item] = key0

                df0["CHAN_BUND"] = df0["chan0"].apply(lambda x: di1[x] if x in di1 else None)
                self.data[df0_peaks0] = dc(df0)

#+END_SRC
